# API de Importa√ß√£o de Leis

API para importar e processar leis de fontes oficiais, com suporte a upload de documentos Word e scraping por URL sob demanda, estruturando automaticamente o conte√∫do legal.

## üöÄ Funcionalidades

- **Scraping por URL (on-demand)**: Extra√ß√£o de leis via endpoint informando a URL
- **Upload de Documentos**: Processamento de arquivos Word (.doc/.docx)
- **Estrutura√ß√£o Hier√°rquica**: Organiza√ß√£o autom√°tica em artigos, par√°grafos, incisos, al√≠neas e itens
- **Cache Inteligente**: Sistema de cache para otimizar performance
- **API RESTful**: Endpoints bem documentados com Swagger
- **Valida√ß√£o Robusta**: Valida√ß√£o de dados com Zod
- **Tratamento de Erros**: Sistema completo de tratamento de erros
- **Testes Automatizados**: Cobertura de testes com Jest

## üèóÔ∏è Tecnologias

- **Node.js** + **TypeScript**
- **Express.js** - Framework web
- **Prisma** - ORM e gerenciamento de banco de dados
- **Playwright** - Automa√ß√£o de browser para scraping
- **Cheerio** - Parser HTML/XML
- **Mammoth** - Processamento de documentos Word
- **Zod** - Valida√ß√£o de schemas
- **Jest** - Framework de testes
- **Swagger** - Documenta√ß√£o da API

## üìã Pr√©-requisitos

- Node.js 18+ 
- npm ou yarn
- PostgreSQL (ou SQLite para desenvolvimento)

## üõ†Ô∏è Instala√ß√£o

1. **Clone o reposit√≥rio**
```bash
git clone <url-do-repositorio>
cd lei-scraper-api
```

2. **Instale as depend√™ncias**
```bash
npm install
```

3. **Configure as vari√°veis de ambiente**
```bash
cp .env.example .env
```

Edite o arquivo `.env` com suas configura√ß√µes:
```env
# Banco de dados
DATABASE_URL="postgresql://usuario:senha@localhost:5432/leis_db"

# Servidor
PORT=3000
NODE_ENV=development

# JWT (se necess√°rio para autentica√ß√£o futura)
JWT_SECRET=seu_jwt_secret_aqui

# Cache (opcional)
REDIS_URL=redis://localhost:6379
```

4. **Configure o banco de dados**
```bash
# Gerar o cliente Prisma
npx prisma generate

# Executar migra√ß√µes
npx prisma migrate dev

# (Opcional) Visualizar o banco
npx prisma studio
```

## üöÄ Execu√ß√£o

### Desenvolvimento
```bash
npm run dev
```

### Produ√ß√£o
```bash
npm run build
npm start
```

### Testes
```bash
npm test
```

## üìö Documenta√ß√£o da API

Ap√≥s iniciar o servidor, acesse:
- **Swagger UI**: `http://localhost:3000/api-docs`
- **Health Check**: `http://localhost:3000/api/health`

## üîó Endpoints Principais

### Scraping
- `POST /api/scrap` - Fazer scraping de uma URL

### Upload
- `POST /api/upload` - Upload de um arquivo Word
- `POST /api/upload/batch` - Upload de m√∫ltiplos arquivos
- `POST /api/upload/validate` - Validar arquivo sem processar

### Leis
- `GET /api/leis` - Listar leis (com pagina√ß√£o e busca)
- `GET /api/leis/:id` - Obter lei espec√≠fica
- `GET /api/leis/stats` - Estat√≠sticas das leis
- `GET /api/leis/:id/export` - Exportar lei (JSON/texto)
- `DELETE /api/leis/:id` - Deletar lei

## üìù Exemplos de Uso

### Scraping de Lei
```bash
curl -X POST http://localhost:3000/api/scrap \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://www.planalto.gov.br/ccivil_03/leis/l8080.htm"
  }'
```

### Upload de Documento
```bash
curl -X POST http://localhost:3000/api/upload \
  -F "file=@documento.docx"
```

### Listar Leis com Filtros
```bash
curl "http://localhost:3000/api/leis?page=1&limit=10&search=saude&origem=planalto"
```

## üèõÔ∏è Sites Suportados

A API suporta scraping dos seguintes sites oficiais:

- **Planalto**: `planalto.gov.br`
- **C√¢mara dos Deputados**: `camara.leg.br`
- **Senado Federal**: `senado.leg.br`
- **Di√°rio Municipal**: `diariomunicipal.org`

## üìä Estrutura de Dados

As leis s√£o estruturadas hierarquicamente:

```typescript
Lei {
  id: string
  titulo: string
  numero: string
  ementa: string
  dataPublicacao: Date
  origem: string
  url?: string
  artigos: Artigo[]
}

Artigo {
  numero: string
  conteudo: string
  paragrafos: Paragrafo[]
}

Paragrafo {
  numero: string
  conteudo: string
  incisos: Inciso[]
}

Inciso {
  numero: string
  conteudo: string
  alineas: Alinea[]
}

Alinea {
  letra: string
  conteudo: string
  itens: Item[]
}

Item {
  numero: string
  conteudo: string
}
```

## üîß Configura√ß√£o

### Cache
O sistema usa cache em mem√≥ria por padr√£o. Para usar Redis:

```env
REDIS_URL=redis://localhost:6379
```

### Modo Manual-Only
O sistema opera exclusivamente por importa√ß√£o manual:

- Upload de arquivos via `POST /api/upload` e varia√ß√µes
- Scraping sob demanda via `POST /api/scrap`
- Sem varredura autom√°tica de diret√≥rios locais e sem scripts de importa√ß√£o autom√°tica

### Logging
Configure o n√≠vel de log em `src/utils/logger.ts`:

```typescript
const logLevel = process.env.LOG_LEVEL || 'info';
```

## üß™ Testes

A aplica√ß√£o possui testes abrangentes:

- **Unit√°rios**: Testam fun√ß√µes individuais
- **Integra√ß√£o**: Testam fluxos completos
- **Middlewares**: Testam valida√ß√£o e tratamento de erros

```bash
# Executar testes espec√≠ficos
npm test -- --testPathPattern=textParser

# Executar com verbose
npm test -- --verbose

# Gerar relat√≥rio de coverage
npm run test:coverage
```

## üöÄ Deploy

### Docker
```dockerfile
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production
COPY . .
RUN npm run build
EXPOSE 3000
CMD ["npm", "start"]
```

### Vari√°veis de Ambiente para Produ√ß√£o
```env
NODE_ENV=production
DATABASE_URL=postgresql://...
PORT=3000
JWT_SECRET=strong_secret_here
```

## ü§ù Contribui√ß√£o

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudan√ßas (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo `LICENSE` para mais detalhes.

## üÜò Suporte

Para suporte e d√∫vidas:

1. Verifique a documenta√ß√£o da API em `/api-docs`
2. Consulte os logs da aplica√ß√£o
3. Abra uma issue no reposit√≥rio

## üìà Roadmap

- [ ] Melhorias no scraping por URL (robustez, normaliza√ß√£o, toler√¢ncia a layout)
